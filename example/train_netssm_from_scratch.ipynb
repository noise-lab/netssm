{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noise-lab/netssm/blob/main/example/train_netssm_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "mATgDQmDyvUB"
      },
      "id": "mATgDQmDyvUB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook covers how to use the NetSSM repository to train a new NetSSM model from scratch on toy PCAP data, including the steps of preprocessing, tokenization, and finally training. It also demonstrates how to convert the raw generation outputted by NetSSM to a parsable PCAP format.\n",
        "\n",
        "NetSSM can be used to train on, and generate both **single and multi-flow network traffic sessions**. The process for either type of data/generation is exactly the same, and follows what is detailed in the notebook below."
      ],
      "metadata": {
        "id": "iImzG0m_y2Mn"
      },
      "id": "iImzG0m_y2Mn"
    },
    {
      "cell_type": "markdown",
      "id": "1f7eff23-4d8f-4d59-9082-dc8aa7785187",
      "metadata": {
        "id": "1f7eff23-4d8f-4d59-9082-dc8aa7785187"
      },
      "source": [
        "# Setup dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup uv"
      ],
      "metadata": {
        "id": "IRwpOjDOYwbI"
      },
      "id": "IRwpOjDOYwbI"
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -LsSf https://astral.sh/uv/install.sh | sh"
      ],
      "metadata": {
        "id": "GF1LF5rZYyTs"
      },
      "id": "GF1LF5rZYyTs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup repo"
      ],
      "metadata": {
        "id": "NIplE8D_0f8Z"
      },
      "id": "NIplE8D_0f8Z"
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/noise-lab/netssm.git"
      ],
      "metadata": {
        "id": "DKv4qeFE0aPK"
      },
      "id": "DKv4qeFE0aPK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python dependencies"
      ],
      "metadata": {
        "id": "ezu36f3R0n6J"
      },
      "id": "ezu36f3R0n6J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If asked to restart the runtime to use newly installed versions, this message can be dismissed."
      ],
      "metadata": {
        "id": "QkD589ti8I4W"
      },
      "id": "QkD589ti8I4W"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = os.path.join(os.getcwd(), \"netssm\")\n",
        "os.chdir(base_dir)\n",
        "! uv sync"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TzxFr740Y8p1"
      },
      "id": "TzxFr740Y8p1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install mamba-ssm and causal-conv1d"
      ],
      "metadata": {
        "id": "ZxRnK9Ftas_m"
      },
      "id": "ZxRnK9Ftas_m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To speed things up, directly install the wheel for both packages by finding the relevant information for our system (CUDA version and ABI)."
      ],
      "metadata": {
        "id": "sj_rlPWla8Y6"
      },
      "id": "sj_rlPWla8Y6"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import subprocess\n",
        "\n",
        "nvcc_cmd = [\"/usr/local/cuda/bin/nvcc\", \"--version\"]\n",
        "nvcc_result = subprocess.run(nvcc_cmd, capture_output=True, text=True)\n",
        "match = re.search(r\"release (\\d+)\", nvcc_result.stdout)\n",
        "cuda_major_version = match.group(1) if match else \"Unknown\"\n",
        "\n",
        "torch_cmd = [\".venv/bin/python\", \"-c\", \"import torch;print(torch._C._GLIBCXX_USE_CXX11_ABI)\"]\n",
        "torch_result = subprocess.run(torch_cmd, capture_output=True, text=True)\n",
        "use_cxx11_abi = torch_result.stdout.strip().upper()\n",
        "\n",
        "mamba_ssm_whl = f\"mamba_ssm-2.2.6.post3+cu{cuda_major_version}torch2.7cxx11abi{use_cxx11_abi}-cp310-cp310-linux_x86_64.whl\"\n",
        "causalconv_whl = f\"causal_conv1d-1.5.4+cu{cuda_major_version}torch2.7cxx11abi{use_cxx11_abi}-cp310-cp310-linux_x86_64.whl\"\n",
        "\n",
        "subprocess.run([\"wget\", f\"https://github.com/state-spaces/mamba/releases/download/v2.2.6.post3/{mamba_ssm_whl}\"])\n",
        "subprocess.run([\"wget\", f\"https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.5.4/{causalconv_whl}\"])\n",
        "\n",
        "! uv pip install {mamba_ssm_whl} --python .venv\n",
        "! uv pip install {causalconv_whl} --python .venv"
      ],
      "metadata": {
        "id": "0a7yxlY9Zqvq"
      },
      "id": "0a7yxlY9Zqvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Go"
      ],
      "metadata": {
        "id": "ZyV4in_52_vK"
      },
      "id": "ZyV4in_52_vK"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "! rm -rf go1.24.2.linux-amd64.tar.gz\n",
        "! wget https://go.dev/dl/go1.24.2.linux-amd64.tar.gz\n",
        "! rm -rf /usr/local/go && tar -C /usr/local -xzf go1.24.2.linux-amd64.tar.gz\n",
        "os.environ['PATH'] += ':/usr/local/go/bin'\n",
        "sys.path.append('/usr/local/go/bin')"
      ],
      "metadata": {
        "id": "9nNM1dMI2z7T"
      },
      "id": "9nNM1dMI2z7T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup libpcap"
      ],
      "metadata": {
        "id": "Vez9Ycw24EKV"
      },
      "id": "Vez9Ycw24EKV"
    },
    {
      "cell_type": "code",
      "source": [
        "! apt update\n",
        "! apt install -y libpcap-dev tshark"
      ],
      "metadata": {
        "id": "6uvEywhE4DDH"
      },
      "id": "6uvEywhE4DDH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fad2703f-5856-4c6a-8556-e91289eafadc",
      "metadata": {
        "id": "fad2703f-5856-4c6a-8556-e91289eafadc"
      },
      "source": [
        "# Preparing training data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcd6dd5e-ca4d-4c24-8afa-f023ba41ede0",
      "metadata": {
        "id": "bcd6dd5e-ca4d-4c24-8afa-f023ba41ede0"
      },
      "source": [
        "Creating training data for NetSSM consists of three steps:\n",
        "  1. Preprocessing PCAPs to raw training samples\n",
        "  2. Creating a custom tokenizer\n",
        "  3. Tokenizing the raw training samples\n",
        "\n",
        "In this notebook, we will be using sample PCAPs representing traffic from various video streaming services, sourced from the [nPrint project](https://nprint.github.io)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "major-broadcast",
      "metadata": {
        "id": "major-broadcast"
      },
      "source": [
        "## Preprocessing PCAPs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "absolute-grammar",
      "metadata": {
        "id": "absolute-grammar"
      },
      "source": [
        "NetSSM trains on sequences of the raw bytes of packets, represented in a string-based format. We provide a preprocessor written in Go, that converts PCAPs to this representation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09b6f53-6522-4052-afbe-ad009b73a684",
      "metadata": {
        "id": "e09b6f53-6522-4052-afbe-ad009b73a684"
      },
      "source": [
        "### Build the preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "individual-flower",
      "metadata": {
        "id": "individual-flower"
      },
      "source": [
        "The following commands build the Go preprocessor to a binary called `netssm_preprocessor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf55613-4c74-4249-8221-72288964bf74",
      "metadata": {
        "id": "9bf55613-4c74-4249-8221-72288964bf74"
      },
      "outputs": [],
      "source": [
        "example_dir = os.path.join(base_dir, \"example\")\n",
        "preprocessing_dir = os.path.join(base_dir, \"preprocessing\")\n",
        "os.chdir(preprocessing_dir)\n",
        "! go mod init netssm_preprocessor\n",
        "! go mod tidy\n",
        "! go build\n",
        "os.chdir(example_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e04669f-2565-4715-b679-a128aab0a61b",
      "metadata": {
        "id": "3e04669f-2565-4715-b679-a128aab0a61b"
      },
      "source": [
        "### Run the preprocessor on sample data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "permanent-zealand",
      "metadata": {
        "id": "permanent-zealand"
      },
      "source": [
        "Our sample data for this example lives at `input`, and contains four PCAP files corresponding to video streaming data for four different platforms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e788926-b5c9-4fb8-a657-5a87ec61f30f",
      "metadata": {
        "id": "3e788926-b5c9-4fb8-a657-5a87ec61f30f"
      },
      "outputs": [],
      "source": [
        "! ls input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "legal-omega",
      "metadata": {
        "id": "legal-omega"
      },
      "source": [
        "The file `labels.csv` contains a mapping of how `netssm_preprocessor` should \"label\" each PCAP for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exciting-jimmy",
      "metadata": {
        "id": "exciting-jimmy"
      },
      "outputs": [],
      "source": [
        "! cat input/labels.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "particular-heater",
      "metadata": {
        "id": "particular-heater"
      },
      "source": [
        "Let's run our preprocessor on the sample data.\n",
        "\n",
        "We pass `input` as the input directory for the processor, `input/labels.csv` for the labels, and write the parsed representations to `output/training_raw.jsonl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77245a23-b6de-4ae3-b371-068651ad0da1",
      "metadata": {
        "id": "77245a23-b6de-4ae3-b371-068651ad0da1"
      },
      "outputs": [],
      "source": [
        "! ../preprocessing/netssm_preprocessor -in-dir input -label-csv input/labels.csv -out output/training_raw.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mexican-louisiana",
      "metadata": {
        "id": "mexican-louisiana"
      },
      "source": [
        "`output/training_raw.jsonl` contains four lines, each corresponding to a training sample parsed from a PCAP in `input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recovered-macintosh",
      "metadata": {
        "id": "recovered-macintosh"
      },
      "outputs": [],
      "source": [
        "! wc -l output/training_raw.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "combined-president",
      "metadata": {
        "id": "combined-president"
      },
      "source": [
        "Each training sample follows the format of `<|label|> RAW_BYTES <|pkt|> RAW_BYTES <|pkt|>...`, where `<|label|>` and `<|pkt|>` are special tokens for the NetSSM model. Let's see an example of what this exactly looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "earlier-channels",
      "metadata": {
        "id": "earlier-channels"
      },
      "outputs": [],
      "source": [
        "! head -c 500 output/training_raw.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we see that the first sample in the `training_raw.jsonl` file we just created, is Amazon streaming traffic."
      ],
      "metadata": {
        "id": "Dl9jMgog84nh"
      },
      "id": "Dl9jMgog84nh"
    },
    {
      "cell_type": "markdown",
      "id": "broad-costs",
      "metadata": {
        "id": "broad-costs"
      },
      "source": [
        "## Creating a custom tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NetSSM treats generating network traffic data as an unsupervised sequence generation problem, where each byte in a packet is represented by a corresponding token. In this section, we'll first create a custom tokenizer for this task, and apply the tokenizer to our preprocessed data prior to model training."
      ],
      "metadata": {
        "id": "P5EI88cr_9mv"
      },
      "id": "P5EI88cr_9mv"
    },
    {
      "cell_type": "markdown",
      "id": "trying-bobby",
      "metadata": {
        "id": "trying-bobby"
      },
      "source": [
        "We next need to create a custom tokenizer that allows NetSSM to understand the representation we created in the previous step. Here, we'll create a custom tokenizer which maps each byte value to a corresponding token ID, and also contains special tokens NetSSM needs to understand the specific data (i.e., video streaming traffic) that we use in this example.\n",
        "\n",
        "We'll use the `create_tokenizer.py` script to do this. Specifically, the following command creates a tokenizer contained in directory `tokenizers/video_streaming_tok` that has special tokens corresponding to the four different video streaming sources we use in our example data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "colonial-judges",
      "metadata": {
        "id": "colonial-judges"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.join(base_dir, \"tokenizers\"))\n",
        "\n",
        "! uv run create_tokenizer.py \\\n",
        "    --special_tokens \"netflix amazon youtube twitch\" \\\n",
        "    --tokenizer_name video_streaming_tok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take a look at `video_streaming_tok/tokenizer.json`, we can see the special tokens we defined, along with the mapping of string values 0 - 255, which will be mapped to their corresponding matching token ID."
      ],
      "metadata": {
        "id": "EwWWXsW19TFJ"
      },
      "id": "EwWWXsW19TFJ"
    },
    {
      "cell_type": "code",
      "source": [
        "! sed -n '42,98 p' video_streaming_tok/tokenizer.json"
      ],
      "metadata": {
        "id": "n0-VNNqK9U8g"
      },
      "id": "n0-VNNqK9U8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing our preprocessed data"
      ],
      "metadata": {
        "id": "h_EAdQML_vxW"
      },
      "id": "h_EAdQML_vxW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll tokenized our data from the processing step prior to giving it to the model, to help speedup the training pipeline.\n",
        "\n",
        "This can be helpful if you are using a cluster with a workload manager (e.g., Slurm) that has a time limit per job, and may preempt jobs after this duration has been reached. In this case, you may desire to use the GPU on a cluster node as efficiently as possible."
      ],
      "metadata": {
        "id": "RNkZGoXCBQvf"
      },
      "id": "RNkZGoXCBQvf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the script at `preprocessing/create_tok_dataset.py` to do this. This script will use the tokenizer provided to it to convert the raw training JSONL data to the tokenized format (stored as .arrow files) expected by NetSSM.\n",
        "\n",
        "In the below example, we'll specify a max sample length of 1,000 tokens, as well as enable padding. This will truncate any training samples to a max length of 1,000 tokens, or pad them to this length if they are shorter. The `max_len` value is dependent on your training hardware, and data. In our paper, we are able to use a max length of 100,000 tokens on a Nvidia A40 with 48GB VRAM, corresponding to multiflow traces comprised of over 1,000 packets. Here, we scale this down to 1,000 to accomodate the 12GB VRAM that the Google Colab T4 GPU has, as well as reduce noise from padding (none of our training PCAPs have more than ~170 packets).\n",
        "\n",
        "We also specify the path to the custom tokenizer we created, as well as the raw input data at `example/output/training_raw.jsonl`, and an output directory."
      ],
      "metadata": {
        "id": "ZXRBhTgXDSht"
      },
      "id": "ZXRBhTgXDSht"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(preprocessing_dir)\n",
        "\n",
        "! uv run create_tok_dataset.py \\\n",
        "    --padding \\\n",
        "    --max_len 1000 \\\n",
        "    --tokenizer ../tokenizers/video_streaming_tok/ \\\n",
        "    --data_path ../example/output/training_raw.jsonl \\\n",
        "    --out_path ../example/output/train_data"
      ],
      "metadata": {
        "id": "x4IJWQ3H9XRR"
      },
      "id": "x4IJWQ3H9XRR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to move on to training a NetSSM model!"
      ],
      "metadata": {
        "id": "SqUVTI2yE1Zs"
      },
      "id": "SqUVTI2yE1Zs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "LBBDeBCFE5_s"
      },
      "id": "LBBDeBCFE5_s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll train a NetSSM model using the `train.py` script in the root directory of the repo. This script can take several arguments, some of which are specified in the command below."
      ],
      "metadata": {
        "id": "MY95vmjRk_nt"
      },
      "id": "MY95vmjRk_nt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training from scratch"
      ],
      "metadata": {
        "id": "Xq8d1Tk7vP0W"
      },
      "id": "Xq8d1Tk7vP0W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`--batch` specifies the batch size, or number of that are presented to the model to train on at a time. A larger batch size will typically speed up training time. In our paper, we use a batch size of 1 to present NetSSM with the largest session representation possible, disregarding training time.\n",
        "\n",
        "`--output` specifies the output folder/path to save the model checkpoint.\n",
        "\n",
        "`--tokenizer` specifies the location of the tokenizer the model should use. This should be the same path used to tokenize the data in the prior steps.\n",
        "\n",
        "`--data_path` specifies the location of the training dataset. Though in the above steps, we first convert the raw JSONL data to the tokenized version in `.arrow` format, our training script also accepts a path to the raw JSONL file. Keep in mind the notes from above, in that this may take lots of time and can be inefficient use of a GPU node, especially for a large dataset.\n",
        "\n",
        "`--num_epochs` specifies the number of epochs -- passes over the entire dataset -- that the model should train for.\n",
        "\n",
        "`--torch_dtype` specifies the data precision used to represent the tensors during training. In the below example, we use `float16` to be compatible with the T4 GPU, but in our paper use `bfloat16`, a precision only available on newer GPUs."
      ],
      "metadata": {
        "id": "xbX5L-2bmGZw"
      },
      "id": "xbX5L-2bmGZw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There exist a number of other parameters that can be passed to the script, that are not shown here (`learning_rate`, `gradient_accumulation_steps`). These are model hyperparameters that influence how frequently and by how much model parameters are updated during gradient descent."
      ],
      "metadata": {
        "id": "6J9WXVYLnlrQ"
      },
      "id": "6J9WXVYLnlrQ"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(base_dir)\n",
        "\n",
        "! uv run train.py \\\n",
        "    --batch_size 1 \\\n",
        "    --output checkpoints/notebook_example \\\n",
        "    --tokenizer tokenizers/video_streaming_tok \\\n",
        "    --data_path example/output/train_data \\\n",
        "    --num_epochs 30 \\\n",
        "    --torch_dtype float32"
      ],
      "metadata": {
        "id": "R67abfXGE6fm"
      },
      "id": "R67abfXGE6fm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! If you take a look at `checkpoints/notebook_example`, you'll see checkpoint folders:"
      ],
      "metadata": {
        "id": "CCX-qRrPoZNr"
      },
      "id": "CCX-qRrPoZNr"
    },
    {
      "cell_type": "code",
      "source": [
        "! ls checkpoints/notebook_example"
      ],
      "metadata": {
        "id": "c6SySyigonKC"
      },
      "id": "c6SySyigonKC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resuming training from a checkpoint"
      ],
      "metadata": {
        "id": "KlSHBGDsvMal"
      },
      "id": "KlSHBGDsvMal"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training script saves the most recent two checkpoints. Inside each folder there is the model binary itself in PyTorch format, and the training state.\n",
        "\n",
        "These directories are useful for resuming training. Specifically, training can be picked up from a checkpoint using the same `train.py` script, and by keeping the same arguments as used to begin training from scratch.  The script will look at the location provided by `--output` and find the most recent checkpoint to load from. Let's try this out now, to take our `30_epochs` checkpoint, and train for another 20 epochs:"
      ],
      "metadata": {
        "id": "bac51cmAorll"
      },
      "id": "bac51cmAorll"
    },
    {
      "cell_type": "code",
      "source": [
        "! uv run train.py \\\n",
        "    --batch_size 1 \\\n",
        "    --output checkpoints/notebook_example \\\n",
        "    --tokenizer tokenizers/video_streaming_tok \\\n",
        "    --data_path example/output/train_data \\\n",
        "    --num_epochs 50 \\\n",
        "    --torch_dtype float32"
      ],
      "metadata": {
        "id": "9J_jbAdapYD0"
      },
      "id": "9J_jbAdapYD0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we see the model pick up from the 30 epoch checkpoint, and continue training for the specified amount. The output directory also now has the 50 epoch checkpoint:"
      ],
      "metadata": {
        "id": "h19xEg-opdcR"
      },
      "id": "h19xEg-opdcR"
    },
    {
      "cell_type": "code",
      "source": [
        "! ls checkpoints/notebook_example"
      ],
      "metadata": {
        "id": "6_N2vRzyVn7E"
      },
      "id": "6_N2vRzyVn7E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "Ah8IzJGMRhov"
      },
      "id": "Ah8IzJGMRhov"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained a model, let's try generating data with it using the `generate.py` script."
      ],
      "metadata": {
        "id": "pld8cggtpzI6"
      },
      "id": "pld8cggtpzI6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script also takes various arguments:\n",
        "\n",
        "`--prompt` specifies the starting string/prompt for kickstarting generation. Typically, this can be the label special token, or first packet in a flow to generate the remaining packets for.\n",
        "\n",
        "`--model` specifies the path to the model checkpoint to be used to generate with.\n",
        "\n",
        "`--tokenizer` specifies the location of the tokenizer the model should use. This should be the same path used to tokenize the data in the prior steps.\n",
        "\n",
        "`--genlen` specifies the number of tokens to generate (including the tokenized representation passed in the prompt).\n",
        "\n",
        "`--gen_len_pkts` changes the behavior of `--genlen` to instead correspond to the number of packets (e.g., 100 packets).\n"
      ],
      "metadata": {
        "id": "agsb9OVpqBFk"
      },
      "id": "agsb9OVpqBFk"
    },
    {
      "cell_type": "code",
      "source": [
        "! uv run generation/generate.py \\\n",
        "    --prompt \"<|netflix|>\" \\\n",
        "    --model checkpoints/notebook_example/50_epochs \\\n",
        "    --tokenizer tokenizers/video_streaming_tok \\\n",
        "    --genlen 100 \\\n",
        "    --gen_len_pkts \\\n",
        "    --torch_dtype float32"
      ],
      "metadata": {
        "id": "isugPPVQG0HX"
      },
      "id": "isugPPVQG0HX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the raw generated output. By default this will live at `inference/EXP_1/RUN_1`, where `EXP_1` and `RUN_1` can be modified by changing the arguments `experiment_base_dir`, and `experiment` respectively:"
      ],
      "metadata": {
        "id": "xTc-Ikhgr6G9"
      },
      "id": "xTc-Ikhgr6G9"
    },
    {
      "cell_type": "code",
      "source": [
        "! cat inference/EXP_1/RUN_1/generated.txt"
      ],
      "metadata": {
        "id": "ddOpcx7QsAxA"
      },
      "id": "ddOpcx7QsAxA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating using a pretrained model"
      ],
      "metadata": {
        "id": "WtwbyVQ-vAHv"
      },
      "id": "WtwbyVQ-vAHv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now try loading a pretrained checkpoint that was used for our paper, which was trained on a dataset of Netflix multi-flow traffic comprised of 5,882 captures for 30 epochs. Each capture is represented by 238,500 tokens (2,250 packets), with each packet being 106 tokens.\n",
        "\n",
        "We change `experiment_base_dir` and `experiment` such that the output should be at `inference/notebook/netflix`.\n",
        "\n",
        "Additionally, we pass the flag `gen_len_pkts`, which will cause `genlen` to be treated as the number of ***packets*** to generate (i.e., 100), instead of tokens.\n",
        "\n",
        "There also are a number of generation parameters that can influence how the model outputs tokens. In the next cells, we'll use the generation parameters found in the NetSSM paper, for the Semantic Similarity section (5.3):\n",
        "\n",
        "## Generation parameters\n",
        "\n",
        "`--repetition-penalty` penalizes repetition of tokens during generation. Values > 1.0 discourage the model from repeating the same tokens, which can help improve diversity and reduce looping outputs.\n",
        "\n",
        "`--temperature` controls the randomness of predictions by scaling the logits before sampling. Lower values (e.g., < 1.0) make the output more deterministic; higher values increase diversity.\n",
        "\n",
        "`--minp` sets the minimum probability threshold for tokens to be considered during sampling. Tokens with probabilities below this value are filtered out, encouraging more confident predictions.\n",
        "\n",
        "`--topk` limits the sampling pool to the top-k most probable tokens. Only the top-k tokens are considered for sampling, reducing randomness and encouraging more likely continuations.\n",
        "\n",
        "`--topp` enables nucleus (top-p) sampling, where tokens are sampled from the smallest possible set whose cumulative probability exceeds `p`. This allows for dynamic control over token diversity based on probability mass."
      ],
      "metadata": {
        "id": "OqNZl7g_vGm0"
      },
      "id": "OqNZl7g_vGm0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and setup the pretrained checkpoint\n",
        "! gdown 1koMbDyaTi0buF1eoDplqOFtJLX-ssS6a\n",
        "! mv netflix_multi_100k_30_epochs.zip ./checkpoints && cd ./checkpoints && unzip netflix_multi_100k_30_epochs.zip && mv checkpoint-176460 netflix_multi_100k_30_epochs && cd ..\n",
        "\n",
        "# Generate using the checkpoint\n",
        "! uv run generation/generate.py \\\n",
        "    --prompt \"<|netflix|>\" \\\n",
        "    --model \"./checkpoints/netflix_multi_100k_30_epochs\" \\\n",
        "    --tokenizer \"./tokenizers/nm_tokenizer_multi_netflix\" \\\n",
        "    --experiment_base_dir notebook \\\n",
        "    --experiment netflix \\\n",
        "    --genlen 100 \\\n",
        "    --gen_len_pkts \\\n",
        "    --repetition-penalty 1.8 \\\n",
        "    --temperature 0.5 \\\n",
        "    --minp 0.0 \\\n",
        "    --topp 0.9 \\\n",
        "    --topk 25 \\\n",
        "    --torch_dtype float32"
      ],
      "metadata": {
        "id": "KbbRFzyNtDJx"
      },
      "id": "KbbRFzyNtDJx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look again now at the generated output:"
      ],
      "metadata": {
        "id": "OOMUzEd7vkFP"
      },
      "id": "OOMUzEd7vkFP"
    },
    {
      "cell_type": "code",
      "source": [
        "! cat inference/notebook/netflix/generated.txt"
      ],
      "metadata": {
        "id": "D0qfW2LYvlxE"
      },
      "id": "D0qfW2LYvlxE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting raw generation to a PCAP"
      ],
      "metadata": {
        "id": "XvtMrO5evtu8"
      },
      "id": "XvtMrO5evtu8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you'll want to convert the raw text output to a PCAP for it to be useful. We can do this using the `conversion.py` script like so:"
      ],
      "metadata": {
        "id": "b70ej-egv0XW"
      },
      "id": "b70ej-egv0XW"
    },
    {
      "cell_type": "code",
      "source": [
        "! uv run generation/conversion.py inference/notebook/netflix/generated.txt inference/notebook/netflix/generated.pcap"
      ],
      "metadata": {
        "id": "bXujsjfzwO04"
      },
      "id": "bXujsjfzwO04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script uses `scapy` to build the binary PCAP from the raw text output. We can confirm that the synthetic data is parsable:"
      ],
      "metadata": {
        "id": "onXUAun3wwOp"
      },
      "id": "onXUAun3wwOp"
    },
    {
      "cell_type": "code",
      "source": [
        "! capinfos inference/notebook/netflix/generated.pcap"
      ],
      "metadata": {
        "id": "5mu1sRjYw4Mt"
      },
      "id": "5mu1sRjYw4Mt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! tshark -r inference/notebook/netflix/generated.pcap"
      ],
      "metadata": {
        "id": "Vcn03NjIVvZq"
      },
      "id": "Vcn03NjIVvZq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}